\documentclass[a4paper,12pt,final]{article} %Papierformat, Schriftgröße, BadBox markieren Bilder nich anzeigen (draft/final)
\usepackage[utf8]{inputenc}
\usepackage{lmodern,textcomp} %textfont deutlich runder/schöner + Eurozeichen richtig
%\renewcommand*\familydefault{\sfdefault} %Serifenlos
\usepackage[onehalfspacing]{setspace} %Zeilenabstand
\usepackage[left=2.6cm, right=2.6cm, top=3cm, bottom=3cm]{geometry} %Seitenränder
\usepackage{graphicx} %bilder (includegraphics)
\usepackage{todonotes}
\usepackage[hang]{footmisc} % Fußnoten 2. Zeile einrücken
\usepackage{float} %Positionierung von Bildern etc. [h] 
\usepackage{booktabs} %Tabellen
\usepackage{longtable}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{siunitx} %Zahlen zb exp oder einheiten
\usepackage{csquotes} %sonst bringt babble n Fehler
\usepackage[backend=biber, %Verarbeitet die BibTex datei und aktualisiert sich mit F11
			style=numeric-comp, %macht nummern aus den Quellen?!?
			sorting=none,]{biblatex} %stell sortiern nach Namen der Autoren aus
\DefineBibliographyStrings{ngerman}{ %macht aus u.a. -> et al.
   andothers = {{et\,al\adddot}}, 
}
\bibliography{Literatur.bib}  % Literatur verzeichniss
\usepackage[english,ngerman]{babel}  % The last language in the list is the primary one.
\usepackage{pdfpages} %zum Anhängen von PDF's
\usepackage[pdfusetitle, colorlinks, linkcolor=black, citecolor=black, urlcolor=gray]{hyperref}  % Add clickable links for figures etc. Comment out for printing if desired.
% \hypersetup{
%     colorlinks=false,
% 	}
\numberwithin{equation}{section} %nummeriert Formeln mit Kapitelnummer.Formelnummer ACHTUNG! muss nach hyperref kommen.
\numberwithin{figure}{section} %numeriert Bilder mit Kapitelnummer.Bildnummer ACHTUNG! muss nach hyperref kommen.
\numberwithin{table}{section} %numeriert Tabellen mit Kapitelnummer.Tabelle ACHTUNG! muss nach hyperref kommen.
\setcounter{biburllcpenalty}{9000}% Kleinbuchstaben?? mach die Zeilenumbrüche bei link's
\setcounter{biburlucpenalty}{9000}% Großbuchstaben?? mach die Zeilenumbrüche bei link's
\setcounter{tocdepth}{2} %Tiefe des Literaturverzeichnises, 2=inkl. subsecion so nur eine Seite Inhaltsverzeichnis
\setlength{\parindent}{0pt} %einrücken bei absätzen verhindern
\usepackage{color}
\definecolor{editorGray}{rgb}{0.95, 0.95, 0.95}
\definecolor{editorOcher}{rgb}{1, 0.5, 0} % #FF7F00 -> rgb(239, 169, 0)
\definecolor{editorGreen}{rgb}{0, 0.5, 0} % #007C00 -> rgb(0, 124, 0)
\usepackage{upquote}
\usepackage{listings}
\lstdefinelanguage{JavaScript}{
  morekeywords={typeof, new, true, false, catch, function, return, null, catch, switch, var, if, in, while, do, else, case, break},
  morecomment=[s]{/*}{*/},
  morecomment=[l]//,
  morestring=[b]",
  morestring=[b]'
}

\lstdefinelanguage{HTML5}{
        language=html,
        sensitive=true, 
        alsoletter={<>=-},
        otherkeywords={
        % HTML tags
        <html>, <head>, <title>, </title>, <meta, />, </head>, <body>,
        <canvas, \/canvas>, <script>, </script>, </body>, </html>, <!, html>, <style>, </style>, ><
        },  
        ndkeywords={
        % General
        =,
        % HTML attributes
        charset=, id=, width=, height=,
        % CSS properties
        border:, transform:, -moz-transform:, transition-duration:, transition-property:, transition-timing-function:
        },  
        morecomment=[s]{<!--}{-->},
        tag=[s]
}

\lstset{%
    % Basic design
    backgroundcolor=\color{editorGray},
    basicstyle={\small\ttfamily},   
    frame=l,
    % Line numbers
    xleftmargin={0.75cm},
    numbers=left,
    stepnumber=1,
    firstnumber=1,
    numberfirstline=true,
    % Code design   
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{darkgray}\ttfamily,
    ndkeywordstyle=\color{editorGreen}\bfseries,
    stringstyle=\color{editorOcher},
    % Code
    language=HTML5,
    escapechar=§, 
    alsolanguage=JavaScript,
    alsodigit={.:;},
    tabsize=2,
    showtabs=false,
    showspaces=false,
    showstringspaces=false,
    extendedchars=true,
    breaklines=true,        
    % Support for German umlauts
    literate=%
    {Ö}{{\"O}}1
    {Ä}{{\"A}}1
    {Ü}{{\"U}}1
    {ß}{{\ss}}1
    {ü}{{\"u}}1
    {ä}{{\"a}}1
    {ö}{{\"o}}1
}

\usepackage{fancyhdr} %Kopf/Fußzeile
\pagestyle{fancy}
\setlength{\headheight}{33.1pt}
%\setlength{\footheight}{30pt}
\lhead{\nouppercase{\leftmark}}
\rhead{\includegraphics[width=2.5cm]{Bilder/logo.pdf}}
\lfoot{Abt \& Girke}
\cfoot{}
\rfoot{\thepage}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0.5pt}

\begin{document}
\begin{center} %Titelseite
	\begin{figure}[h]
	\begin{center}
		\includegraphics[width=8cm]{Bilder/logo.pdf} %bilder vertical anordnen zueinander
		\vspace{2cm}
	\end{center}
	\end{figure}
	\begin{Large}	
		\textbf{Entwicklung und Umsetzung einer intuitiven Steuerung für eine Roboterhand durch Erfassen
		der Geste einer menschlichen Hand\linebreak \linebreak Kinematik Labor\\}
		\vspace{1.5cm}
	\end{Large}
	\begin{large}
		des Studienganges Mechatronik und Robotik\linebreak		
		an der Frankfurt University of Applied Sciences\linebreak\linebreak
		von
		\begin{longtable}[b]{c c}
		Peter Abt & 1400337\\
		Felix Girke & 1386888\\
		\end{longtable}
		\vspace{1cm}
		\today\linebreak \linebreak
		\begin{longtable}[b]{p{7.9cm} p{6.9cm}}
		Bearbeitungszeitraum: & 9 Wochen\\
		Betreuer & Prof. Dr. Enno Wagner
		\end{longtable}
	\end{large}
\end{center} 
\thispagestyle{empty}%ende Titelseite
\setcounter{page}{0}
\newpage
\pagenumbering{Roman}
\markboth{Selbstständigkeitserklärung}{}
\section*{Selbstständigkeitserklärung}
\addcontentsline{toc}{section}{Selbstständigkeitserklärung} %Fügt das zum Inhaltsverzeichnis hinzu,
Wir versicheren hiermit, dass wir die Projektarbeit mit dem Thema: \glqq Entwicklung und Umsetzung einer intuitiven Steuerung für eine Roboterhand durch Erfassen
der Geste einer menschlichen Hand \grqq\ , selbst\-ständ\-ig verfasst und keine anderen als die angegebenen Quellen und Hilfsmittel benutzt haben.\vspace{2.5cm}
\begin{longtable}{p{5.5cm} p{3cm} p{6cm}}
 Frankfurt a. M., \today& & \\
 \cline{1-1} \cline{3-3}
 Ort, Datum& &Unterschrift (Abt)\\
\end{longtable}
\vspace{0.5cm}
\begin{longtable}{p{5.5cm} p{3cm} p{6cm}}
 Frankfurt a. M., \today& & \\
 \cline{1-1} \cline{3-3}
 Ort, Datum& &Unterschrift (Girke)\\
\end{longtable}
\addtocounter{table}{-1} % wird nicht mitgezählt bei der Tabellen Nr.
\newpage
%\markboth{Zusammenfassung}{}
%\section*{Zusammenfassung}
%\addcontentsline{toc}{section}{Zusammenfassung} %Fügt das zum Inhaltsverzeichnis hinzu
%\todo{brauchen wir das hier?}
%\newpage
\tableofcontents %Inhaltsverzeichis
\newpage
\phantomsection  %korregiert die links auf die Seite
\addcontentsline{toc}{section}{Abbildungsverzeichnis und Tabellenverzeichnis} %Fügt das Abbildungs zum Inhaltsverzeichnis hinzu
\listoffigures %abbildungsverzeichnis
% \listoftables %tabellenverzeichnis
\newpage
\pagenumbering{arabic}
\section[Einleitung ]{Einleitung} \sectionmark{Einleitung}
Endeffektoren sind \glqq gewöhnlich das letzte Glied der kinematischen Kette einer Hand\-hab\-ungs\-ein\-richt\-ung\grqq\ \cite[S.302]{Hesse2013}. 
Schon früh wurde versucht mit Endeffektoren die men\-sch\-lich\-en Hand nachzubilden. Lange Zeit war diese Aufgabe zu komplex und es wurden Endeffektoren auf eine Aufgabe spezialisiert \cite[S.9]{VDI1990}.
Aber aufgrund der technischen Fortschritte der letzten Jahrzehnte gelingt es immer besser diese Komplexität zu be\-herr\-sch\-en.
So wurde 2013 mit der \glqq SVH 5-Finger-Hand\grqq\ die erste serienreife 5-Finger-Hand mit 9 Motoren von der Schunk GmbH \& Co. KG vorgestellt \cite[S.57]{Wolf2016}.
Durch die Arbeit vorheriger Gruppen, ist auch an der University of Applied Sciences in Frankfurt eine Roboterhand entwickelt und gebaut worden (Abbildung \ref{fig:RoboHand}), welche sich über einen Mikrocontroller steuern lässt.
\begin{figure}[H]
	\begin{center}
		\includegraphics[height=8cm]{Bilder/RoboHandVorne.png}
		\vspace{0.5cm}
		\includegraphics[height=8cm]{Bilder/RoboHandHinten.png}
		\caption{Die zu steuernde \glqq Frankfurter\grqq\ Roboter Hand}
		\label{fig:RoboHand}
	\end{center}
\end{figure}
Diese Roboterhand besteht aus drei Fingern und einem Daumen. Die einzelnen Finger werden über Bowdenzüge und Linearmotoren bewegt. Der Daumen kann rotieren, so dass ein Pinzettengriff mit jedem einzelnen der Finger möglich ist.
Die Ansteuerung geschieht über einen Arduino Mikrocontroller und vier verschiedene Knöpfe. Dies erfordert ein hohes Maß an Einarbeitung und Konzentration der bedienenden Person.
Deshalb wird versucht mit dieser Arbeit eine Ansteuerung zu schaffen welche intuitiver und einfacher ist. Hierzu soll versucht werden die Bewegung der Hand der bedienenden Person auf\-zu\-zeich\-nen und als Eingangssignal zu verwenden.
Besonderen Wert soll hierbei auf die Stärke der Hand -- den Pinzettengriff -- gelegt werden.
\newpage
\section{Stand der Technik}

Die Echtzeit-Erkennung von Handbewegungen ist für die Steuerung von humanoiden Händen von Essenz. Die komplexen Bewegungsabläufe der menschlichen Hand lassen sich aufgrund der großen Anzahl an Fingersegmenten und Freiheitsgraden nur mit hohem Aufwand erfassen. Und eine Steuerung mittels Joysticks o.ä. ist ungeeignet, da es wenig intuitiv ist.
Erste Versuche die Bewegungsabläufe der Hand aufzunehmen wurden mithilfe von in Handschuhen eingebauten Biegesensoren \cite{FlexSensor} und Lagesensoren durchgeführt.  Die zu dieser Zeit boomende Computerspielindustrie griff die Idee schnell auf und brachte den, technisch vereinfachten, PowerGlove \cite{PowerGlove} (Abbildung \ref{fig:PowerGlove}) auf den Markt. Heute sind verschiedene Firmen im Markt die professionelle Systeme vertreiben wie CyberGlove Systems \cite{CyberGlove} oder Cobra Glove \cite{CobraGlove}. Diese bedienen sich meist der Erfassung der Fin\-ger\-pos\-ition\-en durch eine Kombination von mehreren an den Fingern angebrachten Inertial Measurment Units (IMUs) und Biegesensoren.
\begin{figure}[H]
	\begin{center}
		 \includegraphics[width=8cm]{Bilder/nintendo-power-glove.jpg}
		 \caption{PowerGlove von Nintendo \cite{NintendoGlove}}
		 \label{fig:PowerGlove} 
	\end{center}
\end{figure}
Handschuhe haben im allgemeinen einige Nachteile die sie mit sich bringen. Der an und Abziehvorgan ist umständlich, die Größe des Handschuhes muss stimmen, Des\-in\-fek\-tions\-maß\-nahmen sind kompliziert.

Alternativ werden Handbewegungen auch mit Bewe\-gungs\-er\-kenn\-ungs\-sys\-temen durch\\ Marker und IR-Kamera\-systemen aufgezeichnet. Über Triangolie die Position der ein\-zel\-nen Markerpunkte berechnet. Hier ist die Firma VICON ein Vorreiter auf dem Markt.

Auch markerlose Kamerasysteme zur Bewegungserkennung existieren wie durch z.B. die Kinect Kamera ermöglicht.


\newpage
\section{Mögliche Konzepte}
Ziel ist es die Bewegungen der Hand aufzunehmen.
Um eine ideale Lösung zu finden, werden verschiedene Lösungsmöglichkeiten skizziert und anschließend bewertet.
Wichtig für die Bewertung sind die Genauigkeit des Systems, die Flexibilität zwischen ver\-schie\-den\-en Anwendern und die entstehenden Kosten. 
\subsection{Bowdenzug über Finger}
Die komplexe, mehrdimensionale Bewegung der Finger kann über einen Bowdenzug abgegriffen werden und in eine lineare Bewegung verwandelt werden.
Inspiration hierzu ist die bestehende Roboter Hand (Abbildung \ref{fig:RoboHand}), welche die einzelnen Finger durch solche Bowdenzüge steuert.
Wird diese lineare Bewegung gemessen, so kann bestimmt werden wie stark ein Finger gekrümmt wird. 
\begin{figure}[H]
	\begin{center}
		\includegraphics[width=10cm]{Bilder/HandPhoto1.pdf}
		\caption{Bautenzug über die Hand für die Finger}
		\label{fig:HandFinger}
	\end{center}
\end{figure}
In Abbildung \ref{fig:HandFinger} ist zu erkennen wie ein solcher Bowdenzug (orange) für die Finger angebracht werden könnte.
Das schwarze Objekte an der Fingerspitze ist der Be\-fest\-ig\-ungs\-punkt des Bowdenzug, die schwarzen Kreise auf den Gelenken sind Führungen, damit der Bowdenzug nicht von den Fingern rutscht.
Hellgrün ist die druckfeste Hülle des Bowdenzugs welche verhindert, dass die Bewegungen des Handgelenks in die Messung des Bowdenzugs eingeht.
Am rechten Ende des Bowdenzug befindet sich für jeden Finger ein lineares Potentiometer (dunkel Blau), welches von einer Feder in die Nullstellung zurückgezogen wird.
Es muss für jeden zu messenden Finger ein Potentiometer an\-ge\-bracht werden. Durch messen des Widerstandes des Potentiometers kann die Krümmung des Fingers in ein elektrisches Signal umgewandelt werden. Hierfür würde sich zum Beispiel ein Mikrocontroller eignen. 
Um die Punkte alle auf einer Hand zu befestigen eignet sich ein Handschuh. 
\begin{figure}[H]
	\begin{center}
		\includegraphics[width=10cm]{Bilder/HandPhoto2.pdf}
		\caption{Bautenzug über die Hand für den Daumen}
		\label{fig:HandDaumen}
	\end{center}
\end{figure}
Wird für die Messung des Daumens ähnlich vorgegangen (Abbildung \ref{fig:HandDaumen}), so werden die Messdaten wahrscheinlich wenig Aussagekraft haben.
Geht man mit dem Daumen die einzelnen Fingerspitzen des Pinzettengriffs durch so ist die Bewegung zwischen den einzelnen Fingerknochen sehr gering. Die Bewegung findet lediglich zwischen den Hand\-wur\-zel\-knoch\-en und dem ersten Fingerknochen statt.
Diese Bewegung liegt direkt neben der Bewegung des Handgelenks und ist schwieriger voneinander zu trennen. Des Weiteren ist die Bewegung des Daumens beim Pinzettengriff nur sehr gering weshalb  die Un\-ter\-schei\-dung zu welcher Fingerspitze der Daumen sich bewegt schwer ist.\\
Sind alle Bowdenzüge perfekt angebracht, so ist es theoretisch möglich in einer zu\-künf\-tig\-en Arbeit einen Servomotor parallel zu den Potentiometer anzubringen und somit die Bowdenzüge zu spannen.
Hierdurch könnte dem Nutzer simuliert werden, es würde sich ein Gegenstand zwischen den Fingern befinden, da eine Kraft auf die sich schließenden Finger ausgewirkt werden kann.  
\subsection{Biegesensoren DMS}
Die heutzutage erhältlichen Biegesensoren sind wesentlich preiswerter als die ersten ihrer Art. Sie basieren nicht mehr auf dem Prinzip eines Lichwellenleiters sondern auf der Änderung der Leitfähigkeit von Materialien durch deren Biegung (vgl. Abb. \ref{fig:FlexSensor}). Somit ermöglichen sie eine einfache Möglichkeit den Gebogenheitsgrad eines einzelnen Fingers zu bestimmen. Als schwieriger erweist sich jedoch die Positionsbestimmung des Daumens. Dieser kann sich auch unabhängig seines Biegegrades auf dem unteren Sattelgelenk in zwei Freiheitsgraden bewegen. Ein Biegesensor reicht nicht um z.B. den Pinzettengriff zwischen Daumen und Zeigefinger und zwischen Daumen und Mittelfinger zu unterschieden.


\begin{figure}[ht]
	\begin{center}
		\includegraphics[width=6cm]{Bilder/Flex_Sensor.jpg}
		\caption{auf Leitfähigkeit basierender Biegesensor, lizensiert unter \href{https://creativecommons.org/licenses/by/2.0/}{CC BY 2.0}}
		\label{fig:FlexSensor}
	\end{center}
\end{figure}

\subsection{Image Processing}
Mithilfe des Einsatzes von moderner Bildverarbeitung und künstlicher Intelligenz lassen sich viele Probleme der vorherigen Methoden vermeiden. So sind allen vorran die be\-nö\-tig\-ten Investitionen nahe Null. Lediglich ein PC sowei eine passende Webcam sind bereits aussreichen um die Positionserkennung zu ermöglichen. Ausserdem fallen um\-ständ\-liches an und abziehen eines sensiblen Handschuhes so wie der damit verbundene verkabelungs Aufwand weg. Dank frei verfügbarer, vor-trainierter Machine Learning-Modelle lassen sich selbst komplexe Objekte mit geringem Rechenaufwand erkennen \cite{MediaPipe}. 


\subsection{Entscheidung}
% Bowdenzug negativ handschuh größe, Handschuh verrutscht leicht, Daumen kaum/ nicht messbar, (aufwendig), 

Die erste mechanische Idee mit dem Bowdenzug besticht zwar durch die Möglichkeit Feedback in Form eines Wiederstandgefühls zu simulieren. Allerdings gestaltet sich der Aufbau mithilfe eines Handschuhs schwierig und würde bei nicht exakt passender Hand(-schuh)-größe sehr leicht verrutschen. Ein weiteres Problem die Positionsbestimmung des Daumens. Aufgrund des Sattelgelenks kann eine Unterscheidung zwischen Pinzetten\-griffen für verschiedene Finger nicht erfolgen. Selbiges gilt für den Ansatz mit Biege\-sensoren. Der Aufbau wäre zwar leichter und weniger anfällig zu Verrutschen, die genaue Erkennung der Handgesten ist aber aufgrund der vielen Freiheitsgrade nicht möglich.
Daher fällt die Entscheidung auf den Ansatz mit moderner Bildverarbeitung und Masch\-ine-Learning. Die Unterscheidung zwischen verschiedenen Pinzettengriffen ist möglich. Hinzu kommt dass das System unabhängig von der Handgröße für jeden Nutzer funk\-tio\-niert und durch den Verzicht auf Handschuhe keine besonderen Reinigungsmaßnahmen erforderlich sind. Fertig trainierte Modelle sind frei verfügbar und die Genauigkeit der Erkennung sehr hoch.


\newpage
\section{Umsetzung des Konzepts}

Die Wahl der Programmiersprache für den Bildbearbeitungsansatz von Handgesten fiel auf die Sprache Javascript. Es ermöglicht die Verwendung des Programms auf allen Betriebssystemen mit aktuellem Browser. So könnten auch mobile Endgeräte zur Er\-fass\-ung der Handgesten genutzt werden.
Außerdem lässt sich der Zugriff auf die Webcam und das Senden von Informationen über die Netzwerkschnittstelle leicht realisieren. 
Als System zur Erkennung von Handgesten wurde \enquote{MediaPipe Hands} gewählt, dieses ist auch für Javascript verfügbar \cite{MediaPipeHands}.
Zum erleichterten Zugriff auf die Funktionalitäten des MediaPipe-Modells wird die Bibliothek Handsfree eingesetzt \cite{Handsfree}.

\subsection{Code zur Handgestenerkennung und Nutzeroberfläche}
Der Javascriptcode wird über ein HTML-Dokument im Browser geöffnet.
Im HTML-Code werden zuerst, im Header-Element die benötigten JavaScript-Bibliotheken zur Handgestenerkennung geladen und CSS-Style Vorschriften für das Design der Seite de\-fi\-ni\-ert:
 
 \begin{lstlisting}[firstnumber=3]
<head>
    <!-- Include Handsfree.js -->
    <link rel="stylesheet" href="https://unpkg.com/handsfree@8.5.1/build/lib/assets/handsfree.css" />
    <script src="https://unpkg.com/handsfree@8.5.1/build/lib/handsfree.js"></script>
    <!-- Styles -->§\vspace{-0.5em}\\\hspace*{2em}\large ... \setcounter{lstnumber}{27}§
</head>
 \end{lstlisting}

 Anschließend wird die Seitenüberschrift sowie der Webcamfeed eingebettet:
 
\begin{lstlisting}[firstnumber=30]
<div> <!--Title-->
	<h1>Finger tracking</h1>
</div>
<!-- Holds the Video -->
<div id="video-holder"  style="margin: 1cm; width: 1280px; height: 720px"></div> 
\end{lstlisting}

Das letzte Element der Nutzeroberfläche bildet eine Tabelle zur Übersicht der aktuell erkannten Gesten. Jede Zelle enthält ein span-Element welches basierend auf den im Header definierten Styles als Kreis sichtbar wird. Zusätzlich ist jedes span-Element mit einer ID ausgestattet sodass die Farbe des Kreises vom folgenden Javascript aus geändert werden kann.

\begin{lstlisting}[firstnumber=38]
<div> <!--Status Table-->
<table id="coll">
	<thead>
		<tr>
			<th></th>
			<th>Zeigefinger [0]</th>
			<th>Mittelfinger [1]</th>
			<th>Ringfinger [2]</th>
			<th>kleiner Finger [3]</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<th>linke Hand [0]</th>
			<td><center><span class="dot" id="F00"></span></td>
			<td><center><span class="dot" id="F01"></span></td>
			<td><center><span class="dot" id="F02"></span></td>
			<td><center><span class="dot" id="F03"></span></td>
		</tr>
		<tr>
			<th>rechte Hand [1]</th>
			<td><center><span class="dot" id="F10"></span></td>
			<td><center><span class="dot" id="F11"></span></td>
			<td><center><span class="dot" id="F12"></span></td>
			<td><center><span class="dot" id="F13"></span></td>
		</tr>
	</tbody>
</table>
</div>
\end{lstlisting}

Anschließend folgt der Javascript Code. Zuerst wird eine Funktion definiert die das Senden von Get-Requests mit Informationen über die Handgeste erlaubt. In Zeile 32 wird die Adresse, sowie der Port, des LabView-Servers oder localhost, falls Client und Server auf dem gleichen PC laufen, angegeben.
\begin{lstlisting}[firstnumber=67]
<script>
	function httpGet(theUrl) {
	  let xmlHttpReq = new XMLHttpRequest();
      let url = 'http://localhost:80/WebServerHand/GetPinchState'+theUrl; 
	  xmlHttpReq.open("GET", url, false);
	  xmlHttpReq.send(null);
	  return xmlHttpReq.responseText;
	}
</script>
\end{lstlisting}

Es wurde auch die schnellere Übermittlung der Handgestendaten mittels Websocket getestet. Dazu muss ein Websocket angelegt werden.
\begin{lstlisting}[firstnumber=99]
var websocket = new WebSocket("ws://localhost:2323");
\end{lstlisting}
Dies hat jedoch den Nachteil dass eine dauerhafte Verbindung benötigt wird und die Implementierung eines Websocket-Server in Lab-View komplizierter ist. Daher wird im folgenden die robustere Variante mittels Get-Request verwendet.

Um Gebrauch der Handsfree-Bibliothek zu machen wird ein Handsfree Element angelegt und entsprechend initialisiert.  Um den Camerafeed für den Nutzer sichtbar zu machen wird \texttt{showDebug} auf \texttt{true} gesetzt und die ID des Video-Elements übergeben.

Da nur eine Roboterhand gesteuert werden soll wird die Maximale Anzahl an Händen auf 1 gesetzt. Um die Möglichkeit zu haben mehrere Hände zu steuern, wurde der restliche Code auf die Möglichkeit von 2 Handen angepasst. Dabei wird immer die Hand mit der höchsten Sicherheit gewählt. Um falsche Detektionen von Hand-ähnlichen Objekten möglichst gering zu halten wird die minimale Sicherheit auf 90~\% gesetzt. 
Anschießend wird die Handerkennung gestartet (Zeile 96).


\begin{lstlisting}[firstnumber=79]
const handsfree = new Handsfree({
	showDebug: true,
	setup: {
		wrap: {
		$parent: document.querySelector('#video-holder')
		}
	},
	hands:{
		enabled:true,
		// The maximum number of hands to detect [0 - 4]
		maxNumHands: 1,
		// Minimum confidence [0 - 1] for a hand 
		minDetectionConfidence: 0.9,
		// Minimum confidence [0 - 1] for the landmark tracker
		minTrackingConfidence: 0.9
	 }
})
handsfree.start()
\end{lstlisting}

in der Variable \texttt{FState} werden die Zustände der einzelnen Finger gespeichert. Dabei steht 0 für offen und 1 für Pinzettengriff zum Daumen.
\begin{lstlisting}[firstnumber=101]
let FState =[0,0,0,0];
\end{lstlisting}

Im folgenden werden über eine doppelte for-Schleife einmalig die Callbacks für alle Finger, beider Hände definiert.  Das bedeutet sobald die aktive \texttt{handsfree}-Instanz den Zangengriff für einen Finger erkennt wird das entsprechend definierte Callback auf\-ge\-ruf\-en.
In den Callbacks wird die Farbe des entsprechenden Punktes in der oben definierten Tabelle geändert sowie der Status des Fingers in der globalen Variable \texttt{FState} angepasst. 
Anschließend wird ein Get-Request, mit den Finger Zuständen, an den Lab-View Server gesendet.
Alternativ kann durch auskommentieren der Zeilen 112 oder 120 auch ein Websocket an einen geeigneten Empfänger gesendet werden.

\begin{lstlisting}[firstnumber=104]
for (let hand = 0; hand < 2; hand++) {
for (let finger = 0; finger < 4; finger++) {
	handsfree.on(`finger-pinched-start-${hand}-${finger}`, () => {
		const Finger = document.getElementById('F'+hand+finger)
		Finger.classList.add('dotRed')
		if(hand==0){return;} // send only right hand to server
		FState[finger]=1
		httpGet("?finger3="+FState[3]+"&finger2="+FState[2]+"&finger1="+FState[1]+"&finger0="+FState[0])
		//websocket.send("finger"+finger+"=pinched");
	})
	handsfree.on(`finger-pinched-released-${hand}-${finger}`, () => {
		const Finger = document.getElementById('F'+hand+finger)
		Finger.classList.remove('dotRed')
		if(hand==0){return;} // send only right hand to server
		FingerState[finger]=0
		httpGet("?finger3="+FState[3]+"&finger2="+FState[2]+"&finger1="+FState[1]+"&finger0="+FState[0])
		//websocket.send("finger"+finger+"=released");
	})
}
}
\end{lstlisting}

\subsection{LabVIEW}
Für die Roboterhand wird derzeit eine Steuerung in LabVIEW von einem anderen Team entwickelt. 
Deshalb müssen die erfassten Daten nach LabVIEW übertragen werden. Hierfür wird mit LabVIEW ein lokaler Server Programmiert, welcher von der Web\-ober\-fläche per GET-Request angesprochen wird und die entsprechenden Daten übertragen bekommt \cite{NIwebserverTutorial}.
Das angelegte LabVIEW Projekt besteht aus dem WebServerHand und den \\SharedVariables.lvlib (Abbildung \ref{fig:ProjektExplorer}).
\begin{figure}[H]
	\begin{center}
		\includegraphics[width=7cm]{Bilder/ProjektExplorer.png}
		\caption{Übersicht der Dateien des LabVIEW Projektes}
		\label{fig:ProjektExplorer}
	\end{center}
\end{figure}
In dem Server ist eine GET-Methode angelegt namens GetPinchState.vi. Diese besteht aus einem User Panel (Abbildung \ref{fig:LabVIEWuserPanel}), einem Block Diagram (Abbildung \ref{fig:LabVIEWBlockDia}) und einem Connector Pane (Abbildung \ref{fig:LabVIEWConnectorPane}).
\begin{figure}[H]
	\begin{center}
		\includegraphics[width=10cm]{Bilder/UserPanel.png}
		\caption{Aufbau des User Panels in der GET-Methode des LabVIEW Servers}
		\label{fig:LabVIEWuserPanel}
	\end{center}
\end{figure}  
In dem User Panel ist der Block LabVIEW Web Service Request angelegt über diesen kommt der GET Request in LabVIEW an.
Des Weiteren sind für jeden der vier ge\-mes\-sen\-en Finger ein Numeric Control Panel angelegt. Diese sollen den aktuellen Stand der Finger anzeigen.
\begin{figure}[H]
	\begin{center}
		\includegraphics[width=10cm]{Bilder/BlockDiagram.png}
		\caption{Aufbau des Block Diagrams in der GET-Methode des LabVIEW Servers}
		\label{fig:LabVIEWBlockDia}
	\end{center}
\end{figure}
Die beschrieben Blöcke im User Panel werden im Block Diagram miteinander logisch verbunden.
Der Block LabVIEW Web Service Request und ein Query String sind mit dem eigentlichen Server verbunden. Die einzelnen Finger müssen nicht mit dem Server verbunden werden, sie erhalten Ihre Daten durch die Verknüpfung im Connector Pane.
Damit die Werte der Finger außerhalb des Servers verwendet werden können, sind diese mit einer Shared Variable Node verbunden. Somit wird der jeweilige Wert in die globale Variable übernommen und kann von der Ansteuerung der Roboterhand genutzt werden.  
\begin{figure}[H]
	\begin{center}
		\includegraphics[width=5cm]{Bilder/ConnectorPane.png}
		\caption{Aufbau des Connector Panes in der GET-Methode des LabVIEW Servers}
		\label{fig:LabVIEWConnectorPane}
	\end{center}
\end{figure}  
Im Connector Pane sind von oben nach unten die Finger 0 bis 3 als Input verknüpft (Abbildung \ref{fig:LabVIEWConnectorPane}, orange) und der LabVIEW Web Service Request (blau). 
Dadurch ge\-lang\-en die Werte an die richtigen Variablen.\\
Mit einem Rechtsklick auf den WebServerHand im Explorer (Abbildung \ref{fig:ProjektExplorer}) kann der Server gestartet werden. Vorher sollten allerdings diese Einstellungen in der NI-Web\-ser\-ver-Konfiguration vorgenommen werden (Abbildung \ref{fig:EinstellungenWebServer}).
\begin{figure}[H]
	\begin{center}
		\includegraphics[width=8cm]{Bilder/Einstellungen-StartServerNeu.png}
		\caption{Nötige Einstellungen in der NI-Webserver-Konfiguration}
		\label{fig:EinstellungenWebServer}
	\end{center}
\end{figure}
Sind diese Einstellungen vorgenommen und der Server gestartet, so kann über einen Rechtsklick auf die GetPinchState.vi (Abbildung \ref{fig:ProjektExplorer}) die Methoden-URL angezeigt wer\-den (Abbildung \ref{fig:HTTPMethod})
\begin{figure}[H]
	\begin{center}
		\includegraphics[width=12cm]{Bilder/HTTPMethod.png}
		\caption{URL des HTTP-GET Servers}
		\label{fig:HTTPMethod}
	\end{center}
\end{figure}
Zur Nutzung des Webservers muss Lediglich diese URL in einen Browser eingefügt werden und \glqq \{value\}\grqq\ durch einen Wert ersetzt werden.
Anstelle der angegeben IP-Adresse kann auch \glqq localhost\grqq angegeben werden, wenn der Browser und der Server sich auf dem selben PC befinden.
Möchte man den Server auf einem anderen PC laufen lassen, so muss die IP-Adresse durch die des PC's im Netzwerk ersetzt werden.
\subsection{Demonstration}
In dem angehängtem Video (Anhang 3, DemoVideo.mkv), ist auf der linken Seite das User Panel des LabVIEW Webserver zu sehen und auf der rechten Seite die Web\-ober\-fläche (Abbildung \ref{fig:ScreenshotVideo}).
\begin{figure}[H]
	\begin{center}
		\includegraphics[width=12cm]{Bilder/ScreenshotVideo.png}
		\caption{Screenshot aus dem Demovideo im Anhang}
		\label{fig:ScreenshotVideo}
	\end{center}
\end{figure}
Es ist im Videofeed rechts zu erkennen das sich der Mittelfinger und der Daumen berühren. In der Tabelle darunter sieht man das in der Weboberfläche erkannt wurde, dass sich nur diese Finger berühren.
In dem User Panel links ist zu sehen, dass lediglich bei Finger 1 der Wert von 0 auf 1 geändert wurde. Die Berührung wurde also korrekt erkannt und an LabVIEW übermittelt.\\
Das Demonstrationsvideo lässt den Prozess etwas langsam und verzögert wirken. Dies liegt allerdings nur daran das der Testrechner mit einer schwachen und über 10 Jahren alten CPU ausgestattet ist und gleichzeitig den Bildschirm aufzeichnet. 
Ohne Bild\-schirm\-auf\-zeich\-nung läuft der Prozess flüssig und deutlich schneller. 
\newpage
\section{Diskussion}
Wie in den Demonstrationsvideo zu erkennen ist das entworfene Konzept der Hand\-ge\-sten\-er\-ken\-nung mithilfe von Bildverarbeitung und Maschine-Learning ist voll funk\-tions\-fähig.
Die verschiedenen Pinzetten\-griffe können frei von jeglicher Verkabelung vom Nutzer durchgeführt werden und werden vom System zuverlässig erkannt.
Dank der Implementierung über Javascript lässt sich das System auch auf einfacher Hardware wie Mobiltelefonen, einem SoC o.ä. ausführen.\\
Die erfassten Daten können direkt intern oder per Lan- oder WLAN-Schnittstelle zu LabVIEW  oder einem anderen Server gesendet werden. Dadurch ist eine Anbindung an die Steuerungssoftware der Hand flexibel möglich.
Die Übertagung der Daten nach LabVIEW funktioniert zuverlässig und ausreichend schnell.
Diese Daten stehen global in dem LabVIEW Projekt zu Verfügung und können somit einfach in die Steuerung eingebunden werden.\\
Die Kosten für das Projekt sind sehr niedrig bis nicht vorhanden. Sollte die Steuerung der Hand auf einem Laptop laufen, so ist eine Webcam in den meisten Fällen integriert.
Wird ein stationärer PC oder ein SoC wie beispielsweise ein Raspberry Pi eingesetzt, so kostet eine Webcam oder PiCam weniger als 20€, vor allem da keine hohe Auflösung nötig ist.\\
Eines der Hauptziele war es die Steuerung intuitiver zu gestalten als das bisherige System, welches auf vier Knöpfe zur Steuerung setzt. Die neue Steuerung über die Bewegung der eigenen Hand ist für die bedienenden Person sehr einfach.
Der gespiegelte Webcamfeed mit dem eingeblendeten Modell und die Tabelle zu den erkannten Pinzettengriffen er\-leich\-tert die Bedienung, sodass ein Einweisen in das Systems selbst für neue Nutzer kaum notwendig ist.\\
Die bedienende Person muss also lediglich ihre Hand vor die Kamera halten und die Roboterhand imitiert sofort den Pinzettengriff. 
\newpage
\section{Ausblick}
Als anschließende Projekte könnte noch Erweiterungen des Systems folgen. So können theoretisch mehrere Hände in einem Videofeed erkannt werden.
Somit könnte bei\-spiels\-weise eine linke und eine rechte Hand erkannt und gesteuert werden. Dadurch kann eine bedienende Person mehrere Roboter gleichzeitig steuern.\\ 
Die Bildverarbeitung erkennt im Moment lediglich den Pinzettengriff als Aktion und sendet diese an die Steuerung. Da die Position jedes Fingergelenkes erfasst wird (Ab\-bild\-ung \ref{fig:ScreenshotVideo}, rote Punkte) und abrufbar ist, können theoretisch eigene Algorithmen entwickelt werden.
Diese könnten immer die Fingerpositionen erfassen und somit der Roboterhand bereits kleine Fingerbewegungen mitteilen. Dann könnte die Roboterhand nicht nur den Pinzettengriff ausführen sondern fast jegliche Objekte greifen.
Schwierigkeiten hierbei sind allerdings, es muss zunächst eine Kalibrierung auf eine Hand stattfinden, da jede Hand anders groß ist, es muss beachtet werden das sich die Hand von der Kamera weg bewegen kann und dadurch kleiner wird und auch das die Roboterhand anders aufgebaut ist als eine menschliche Hand.
Hierfür müsste ein System entwickelt werden, welches diese Fälle verarbeitet.\\
\begin{figure}[H]
	\begin{center}
		\includegraphics[width=4cm]{Bilder/VIVE.png}
		\caption{VIVE-Tracker für VR der Firma HTC \cite{HTCvive}}
		\label{fig:VIVE}	
	\end{center}
\end{figure}
Da die Roboter Hand an das Ende eines Roboterarmes montiert werden muss um ihre volles Potential auszuschöpfen, wäre es ideal wenn nicht nur die Bewegung der Hände sondern auch die Bewegung der Arme erfasst werden könnte.
Dann könnte eine Person neben der Roboterzelle stehen und alleine durch ihre Bewegung die Roboter Hand zu einem Gegenstand führen und durch die Fingerbewegungen den Gegenstand greifen.
Durch die Dreidimensionalität dieser Bewegung eignet sich hierfür eine Kamera nicht, es muss auf mehrere Kameras oder auf ein System mit auf dem Körper angebrachten Trackern wie den\glqq VIVE Tracker\grqq\ (Abbildung \ref{fig:VIVE}) zurückgegriffen werden.
\newpage
\pagenumbering{Alph}
\phantomsection 
\addcontentsline{toc}{section}{Literaturverzeichnis} %Fügt das Literaturverzn soeichnis zum Inhaltsverzeichnis hinzu
\printbibliography  % Literaturliste ausgeben.
\newpage
\section*{Anhang} \sectionmark{Anhang}
\addcontentsline{toc}{section}{Anhang} %Fügt den Anhang zum Inhaltsverzeichnis hinzu
\begin{enumerate}
	\item Code welcher für die Website benötigt wird (Ordner)
	\item Code des LabVIEW-Projektes (Ordner)
	\item Demonstrations Video DemoVideo.mkv
\end{enumerate}
\newpage

\end{document}